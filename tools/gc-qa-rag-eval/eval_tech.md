# 活字格认证考试 AI 智能测评报告

## 一、测评背景和目标

为了验证人工智能技术在活字格认证考试中的应用效果，我们构建了一套智能测评系统，对活字格认证考试的各个科目进行了全面的 AI 能力测试。

### 测评目标

-   验证 RAG 在活字格专业领域知识方面的准确性
-   对比 RAG/Agent 应用方案的效果差异

### 测试范围

本次测评覆盖了活字格认证考试体系的三个核心科目：

-   **活字格认证工程师-科目一**（基础理论，348 题）
-   **活字格认证工程师-科目二**（实践应用，108 题）
-   **活字格高级认证工程师-科目一**（高级功能，85 题）

## 二、测评方法说明

### 2.1 总体性能表现

根据测评结果数据，三种评测模式在不同科目上的表现如下：

| 考试科目                        | 直接生成答案     | 结合知识库检索   | Agent 自动规划检索   | 提升幅度    |
| ------------------------------- | ---------------- | ---------------- | -------------------- | ----------- |
| **活字格认证工程师-科目一**     | 65.80% (229/348) | 81.03% (282/348) | **88.51%** (308/348) | **+22.71%** |
| **活字格认证工程师-科目二**     | 57.41% (62/108)  | 69.44% (75/108)  | **70.37%** (76/108)  | **+12.96%** |
| **活字格高级认证工程师-科目一** | 52.94% (45/85)   | 65.88% (56/85)   | **74.12%** (63/85)   | **+21.18%** |

### 2.2 关键发现

#### 2.2.1 Agent 模式显著优势

Agent 自动规划检索模式在所有测试科目中均表现最佳：

-   **科目一（基础）**：正确率达到 88.51%，相比直接生成提升 22.71%
-   **科目二（应用）**：正确率达到 70.37%，提升幅度相对较小但仍然显著
-   **高级科目一**：正确率达到 74.12%，在难度最高的科目上仍保持良好表现

#### 2.2.2 知识库检索效果分析

RAG 检索增强相比直接生成均有明显改善：

-   平均提升幅度在 12-15%之间
-   表明知识库构建质量较高，能够有效补充 LLM 的领域知识缺失

#### 2.2.3 难度与性能相关性

观察到随着考试难度增加，所有模式的准确率均有下降：

-   基础科目（科目一）：88.51% → 高级科目：74.12%
-   应用科目（科目二）：70.37%，反映了理论与实践应用间的知识迁移挑战

## 三、技术架构深度分析

### 3.1 核心组件架构

#### 3.1.1 评测引擎（eval.py）

```python
# 核心评测逻辑位于 app/eval.py:41-148
class EvaluationEngine:
    def evaluate_model_by_subject(self, llm, subject, parallel_count,
                                 max_questions, with_context, agentic_mode)
```

**设计亮点：**

-   **多模式统一接口**：通过参数控制不同评测模式，代码复用性强
-   **精确评分机制**：基于正则表达式的答案提取，支持`<答案></答案>`标签格式化
-   **详细日志追踪**：完整的评测过程记录，便于问题诊断和性能调优

#### 3.1.2 LLM 抽象层（llm.py）

系统采用接口抽象设计，支持多种 LLM 服务：

```python
# 抽象接口定义 app/llm.py:11-25
class LLMInterface(ABC):
    @abstractmethod
    def generate_answer(self, question: Question, context: List[Dict] = None) -> str

    @abstractmethod
    def generate_answer_agentic(self, question: Question, kb_client: KnowledgeBaseClient) -> str
```

**技术特色：**

-   **Function Calling 集成**：Agent 模式基于 OpenAI Function Calling 实现自主搜索决策
-   **多轮对话管理**：支持最多 10 轮的迭代式信息收集过程
-   **异常处理机制**：完善的错误处理和降级策略

### 3.2 Agent 自主规划机制深度分析

#### 3.2.1 Function Schema 设计

```python
# app/llm.py:42-57
def _get_search_function_schema(self) -> Dict:
    return {
        "name": "search_knowledge_base",
        "description": "Search the knowledge base（RAG vector database） for relevant information...",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The natural language question query string..."
                }
            },
            "required": ["query"]
        }
    }
```

#### 3.2.2 自主决策流程

1. **问题理解阶段**：LLM 分析题目内容和选项
2. **信息需求评估**：判断是否需要额外信息支持
3. **迭代式检索**：根据理解深度进行多轮精准搜索
4. **综合推理决策**：基于检索到的信息进行最终判断

#### 3.2.3 提示工程优化

系统采用了高度优化的提示模板：

```python
# app/llm.py:101-108
system_prompt = f"""你是一个具有高度自主能动性的问题解决专家。
## 注意事项：
1. 你拥有search_knowledge_base工具，应该像人类专家通过RAG搜索引擎解决复杂问题一样。
2. 最终答案的字母选项请务必使用<答案></答案>标签包裹，如果是多个选项，字母之间用英文逗号分隔。
```

### 3.3 知识库集成架构

#### 3.3.1 向量检索客户端（query.py）

```python
# app/query.py:11-29
def search(self, keyword: str, product: str = None) -> List[Dict]:
    url = f"{self.base_url}/search/"
    payload = {"keyword": keyword}
    if product:
        payload["product"] = product
        payload["mode"] = "chat"
```

**设计特点：**

-   **产品分类检索**：根据题目产品属性（forguncy、wyn、spreadjs）进行精准检索
-   **结果数量控制**：限制返回 8 条最相关结果，平衡信息质量和处理效率
-   **异常处理**：网络异常时返回空结果，不中断评测流程

## 四、性能提升机制分析

### 4.1 Agent 模式成功因素

#### 4.1.1 自适应信息收集

-   **按需检索**：只在需要时进行搜索，避免信息过载
-   **多角度查询**：可以从不同角度多次搜索同一问题
-   **渐进式理解**：通过多轮交互逐步深化对问题的理解

#### 4.1.2 上下文管理优化

-   **消息历史维护**：完整保留对话历史，支持复杂推理链
-   **工具调用记录**：详细记录每次搜索的查询和结果
-   **迭代限制机制**：防止无限循环，确保系统稳定性

### 4.2 评分机制设计

#### 4.2.1 答案提取策略

```python
# app/eval.py:18-26
def _score_answer(self, question: Question, answer: str) -> Tuple[float, str]:
    # 优先提取<答案>...</答案>标签内容进行比较
    matches = re.findall(r"<答案>(.*?)</答案>", answer)
    answer_to_check = matches[-1].strip() if matches else answer.strip()
```

**优势：**

-   **格式化输出**：强制 LLM 使用特定标签，提高答案解析准确性
-   **容错机制**：支持标签缺失情况的降级处理
-   **精确匹配**：避免模糊匹配可能带来的评分误差

## 五、系统局限性与改进建议

### 5.1 当前局限性

#### 5.1.1 评分机制局限

-   **完全匹配要求**：当前采用精确字符串匹配，可能忽略语义等价的正确答案
-   **多选题处理**：选项顺序敏感性可能影响评分准确性

#### 5.1.2 并发控制缺失

```python
# app/eval.py:122-129 - 并发代码被注释
# with concurrent.futures.ThreadPoolExecutor(max_workers=parallel_count) as executor:
#     results = list(executor.map(process_question, enumerate(questions, 1)))
```

当前版本采用序列化处理，影响大规模测试效率。

#### 5.1.3 知识库依赖性

-   **外部服务依赖**：系统严重依赖 localhost:8000 的向量数据库服务
-   **网络异常处理**：虽有异常处理，但不能很好应对服务不可用情况

### 5.2 改进建议

#### 5.2.1 评分机制增强

1. **语义匹配引入**：结合词向量相似度计算，支持语义等价判断
2. **部分分数机制**：多选题按选项正确比例给分
3. **主观题支持**：引入 LLM 作为评分器处理开放性题目

#### 5.2.2 系统可靠性提升

1. **恢复并发处理**：优化资源管理，支持高效并行评测
2. **知识库缓存**：本地缓存频繁查询结果，减少网络依赖
3. **降级策略完善**：知识库不可用时的智能降级机制

#### 5.2.3 Agent 能力扩展

1. **多模态支持**：处理包含图片的题目（当前过滤掉）
2. **推理链可视化**：记录和展示 Agent 的推理过程
3. **个性化策略**：根据不同科目特点调整搜索策略

## 六、结论与展望

### 6.1 核心成果总结

1. **技术创新性**：成功实现了基于 Function Calling 的自主 Agent 测评系统，相比传统 RAG 方法有显著提升
2. **实用价值高**：在活字格认证考试场景中取得了 88.51%的优异成绩，具备商业化应用潜力
3. **架构可扩展**：模块化设计支持快速扩展到其他领域和考试类型

### 6.2 技术价值分析

#### 6.2.1 Agent 模式的优势验证

测评结果清晰证明了 Agent 自主规划检索相比传统方法的优越性：

-   **智能化程度高**：能够根据问题复杂度自适应调整搜索策略
-   **信息利用效率高**：避免无关信息干扰，专注于问题相关的核心知识点
-   **通用性强**：无需针对特定领域手工设计搜索策略

#### 6.2.2 工程实践价值

1. **可复制性强**：完整的代码架构和清晰的接口设计便于推广应用
2. **成本效益显著**：相比人工阅卷，大幅降低成本并提高效率
3. **标准化程度高**：统一的评分标准消除主观判断偏差

### 6.3 未来发展方向

#### 6.3.1 技术演进路径

1. **多模态 Agent**：支持图文混合内容的理解和推理
2. **自适应学习**：基于历史表现动态优化搜索和推理策略
3. **知识图谱集成**：结合结构化知识进行更精准的推理

#### 6.3.2 应用场景扩展

1. **教育评估**：扩展到更多学科和考试类型
2. **专业认证**：应用于各类职业资格认证考试
3. **智能辅导**：基于错题分析提供个性化学习建议

本测评系统代表了 AI 在教育评估领域的重要进展，其 Agent 自主规划的创新设计为构建更智能的教育 AI 系统提供了宝贵经验。随着技术的不断完善，有望在更广泛的教育场景中发挥重要作用。
