# 让 LLM 做低代码考试，谁会胜出？

## 背景介绍

为了探究大语言模型在专业软件领域的知识水平，葡萄城 AI 团队进行了一项测评，让多个主流 LLM 参与了“活字格”低代码平台的认证考试。

本次测评选取了三个模型作为“考生”：

-   **Claude-4-sonnet**
-   **GLM-4.5**
-   **Qwen3** (Qwen3-235B-A22B-2507)

测评的底层技术支撑来源于葡萄城的开源项目 **GC-QA-RAG**，该项目旨在通过检索增强生成（RAG）和 Agent 技术，提升 AI 在专业领域的问答准确性。

## 考试题目介绍

本次测评所用的题目均来自“活字格认证考试体系”，内容覆盖了从基础到高级的三个核心科目，全面考察了对活字格低代码平台的掌握程度。

-   **活字格认证工程师-科目一 (348 题)**：

    -   **难度等级**：基础
    -   **内容类型**：主要考察**基础理论**，包括活字格的各项**概念理解**和**功能特性**。题目形式偏向于对核心知识点的记忆和理解。

-   **活字格认证工程师-科目二 (108 题)**：

    -   **难度等级**：中级
    -   **内容类型**：侧重于**实践应用**，题目多为具体的**操作步骤**和常见的**问题解决方法**。这部分考验的是将理论知识应用于实际场景的能力。

-   **活字格高级认证工程师-科目一 (85 题)**：
    -   **难度等级**：高级
    -   **内容类型**：聚焦于平台的**高级功能**和**深度应用**。题目通常涉及**复杂场景**的设计与实现，对知识的综合运用能力要求很高。

这些真实世界的专业考题，为全面评估 AI 在低代码领域的知识深度和应用能力提供了一个理想的测试基准。

## 测试模式设计

为了全面评估 AI 的能力，测评设计了三种递进式的测试模式：

1.  **模式一：直接生成答案**

    -   **机制**：AI 完全依赖自身训练时学到的知识来直接回答考题。
    -   **目的**：检验大模型在没有外部资料支持下的基础知识水平。

2.  **模式二：结合知识库检索（RAG）**

    -   **机制**：在回答前，AI 可以先从包含活字格官方文档、教程等内容的知识库中搜索相关信息，然后结合搜索结果生成答案。
    -   **目的**：评估引入专业领域知识后，对 AI 回答准确率的提升效果。

3.  **模式三：Agent 自动规划检索**
    -   **机制**：AI 能够自主分析问题，判断是否需要以及如何进行信息检索。它可以根据初步检索的结果，决定是否需要换个关键词进行多轮、多角度的深入检索，模拟人类专家解决问题的过程。
    -   **目的**：测试更智能、更主动的检索策略能达到的最佳效果。

## 测评结果汇总

测试覆盖了活字格从基础到高级的三个考试科目，以下是三个模型在不同模式下的表现汇总。

| 考试科目                         | 模型                | 直接生成答案 | 结合知识库检索 (RAG) | Agent 自动规划检索 | **最大提升** |
| :------------------------------- | :------------------ | :----------- | :------------------- | :----------------- | :----------- |
| **认证工程师-科目一 (基础)**     | **Claude-4-sonnet** | 65.80%       | 81.03%               | **88.51%**         | +22.71%      |
|                                  | **GLM-4.5**         | 61.21%       | 84.20%               | 87.07%             | **+25.86%**  |
|                                  | **Qwen3**           | 67.82%       | 83.05%               | 85.92%             | +18.10%      |
| **认证工程师-科目二 (实践)**     | **Claude-4-sonnet** | 57.41%       | 69.44%               | **70.37%**         | +12.96%      |
|                                  | **GLM-4.5**         | 47.22%       | 64.81%               | 65.74%             | +18.52%      |
|                                  | **Qwen3**           | 51.85%       | 65.74%               | 68.52%             | +16.67%      |
| **高级认证工程师-科目一 (高级)** | **Claude-4-sonnet** | 52.94%       | 65.88%               | **74.12%**         | +21.18%      |
|                                  | **GLM-4.5**         | 57.65%       | 67.06%               | 68.24%             | +10.59%      |
|                                  | **Qwen3**           | 54.12%       | 61.18%               | 68.24%             | +14.12%      |

## 结果分析

从数据中可以得出几个核心结论：

1.  **Agent 模式效果最显著**
    在所有测试中，不论使用哪个具体模型，**Agent 自动规划检索模式**的得分都是最高的。这表明，比起简单的信息检索，让 AI 学会自主规划、迭代提问的检索策略，能更有效地利用知识库，从而显著提升回答的准确率。

2.  **RAG 技术能有效提升准确率**
    对比“直接生成答案”和“结合知识库检索”两列数据，可以发现所有模型在获得外部知识库支持后，成绩都有了大幅提高。例如，GLM-4.5 在基础科目中的分数提升了超过 25 个百分点。这证明了 RAG 技术在专业领域的实用价值：为通用大模型提供精准的、领域内的知识，是其能力提升的关键。

3.  **各模型表现对比**
    综合来看，**Claude-4-sonnet** 在三个科目的 Agent 模式下均取得了最高分，尤其是在难度最高的高级科目中，其 74.12%的正确率显示出较强的综合能力。同时，其他模型如 GLM-4.5 和 Qwen3 在结合 RAG 和 Agent 技术后，表现也获得了很大改善，证明了这种技术框架具有良好的通用性。

## 技术简述

测评表现的背后，是 **GC-QA-RAG** 开源系统所采用的两项核心技术：

1.  **高级 QA 预生成技术**：该技术改变了传统 RAG 简单切割文档的方式，通过模型智能地将原始文档处理成结构化的“问答对(QA)”、“摘要”和“同义问法”等。这相当于为 AI 准备了一套高质量、易于理解和检索的知识材料。

2.  **Agent 自主规划检索**：该技术赋予 AI 规划和执行复杂任务的能力。通过 Function Calling 等机制，AI 可以自主决策何时检索、如何检索，并能根据检索结果调整下一步行动，使整个检索过程更具目的性和效率。

## 测试暴露的问题

尽管 AI 在考试中取得了不错的成绩，但本次测评也清晰地暴露了当前技术方案存在的一些问题与局限性。

1.  **理论与实践的脱节**
    从成绩上看，AI 在回答基础理论（科目一）题目时表现出色，但在处理侧重实际操作步骤（科目二）的题目时，正确率有明显下降。这表明 AI 能够很好地记忆和复述知识点，但在将理论知识转化为具体的、一步步的实践指导方面仍有困难，存在知识迁移的挑战。

2.  **知识的“脆弱性”**
    首先，AI 的回答仍存在 10%至 30%的错误率，在关键业务场景下，错误的答案可能带来风险。其次，AI 的知识是静态的。随着活字格产品功能的迭代更新，如果知识库未能及时同步，AI 的回答很快就会过时，这对其在实际应用中的可靠性提出了持续维护的要求。

3.  **系统与测评方法的局限**
    本次测评也反映出系统本身及测评方法上的一些不足。首先，测评形式相对单一，主要依赖**单选题和多选题**，这种方式无法充分检验 AI 在处理开放性问答或需要进行实际操作配置等更复杂任务时的能力。其次，系统目前**不支持处理包含图片或图表的题目**，即缺乏多模态理解能力，这限制了其在解决需要理解视觉信息的复杂技术问题时的应用范围。

## 结论

本次测评的结果表明，在专业的低代码领域，将大语言模型与先进的 RAG 及 Agent 技术结合，是一条有效的技术路径。

AI 在处理**基础理论知识**时表现出色，最高取得了 88.51%的正确率，展现出成为得力知识助手的潜力。然而，测评也清晰地显示，随着题目难度增加和实践性增强，AI 的准确率出现明显下滑。

这说明，尽管 AI 在特定知识性任务上取得了高分，但在更复杂的实践应用和综合推理方面，与真正的人类专家相比仍存在显著差距。当前，它更适合作为一个高效的“知识检索和查询工具”，而非能够完全独立解决所有问题的“专家”。要让 AI 在专业领域真正落地，还需要在弥补理论与实践差距、应对知识更新和突破系统自身局限等方面持续投入。

不论如何， **Agentic RAG** 这一技术范式的表现还是足够惊艳的。它验证了一条有效的路径：通过为通用大模型配备高质量的外部知识库和智能的检索工具，可以有效解决垂直领域的复杂问题。这对企业在知识管理、技术支持和客户服务等场景的智能化升级，提供了有价值的参考。

对该技术方案感兴趣的读者，可以进一步了解其开源项目：

-   **项目开源地址**: [https://github.com/GrapeCity-AI/gc-qa-rag](https://github.com/GrapeCity-AI/gc-qa-rag)
-   **在线体验 Demo**: [https://ai-assist.grapecity.com.cn/](https://ai-assist.grapecity.com.cn/)
