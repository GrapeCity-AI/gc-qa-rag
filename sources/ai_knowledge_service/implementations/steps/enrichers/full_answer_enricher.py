"""
Full Answer Enricher - Generates detailed answers for each QA pair.

Uses LLM to generate comprehensive, markdown-formatted answers
based on the original document content.
"""

import logging
from typing import Any, Dict, List, Optional

from ai_knowledge_service.abstractions.pipelines.steps import (
    IEnricher,
    ProcessingContext,
)
from ai_knowledge_service.abstractions.observability.context import ObservabilityContext
from ai_knowledge_service.implementations.llm.llm_client import LLMClient


class FullAnswerEnricher(IEnricher):
    """
    Full Answer Enricher - Generates detailed answers for QA pairs.

    Takes the QA pairs generated by QAEnricher and generates more
    comprehensive answers based on the full document content.

    Output is stored in enrichments["full_answer"].
    """

    PROMPT_TEMPLATE = """基于以下<用户问题>，参考<相关文档>，生成一个最符合用户问题的总结性答案，输出为 markdown 格式的文本。

## 用户问题
Q：{question}

## 相关文档
{content}
"""

    def __init__(
        self,
        llm_client: LLMClient,
        logger: Optional[logging.Logger] = None,
    ):
        """
        Initialize the enricher.

        Args:
            llm_client: LLM client for generating full answers.
            logger: Optional logger instance.
        """
        self._logger = logger or logging.getLogger(self.__class__.__name__)
        self._llm_client = llm_client
        self._config: Dict[str, Any] = {}

    @property
    def step_type(self) -> str:
        """Get the step type identifier."""
        return "full_answer_enricher"

    @property
    def enrichment_type(self) -> str:
        """Get the type of enrichment produced."""
        return "full_answer"

    @property
    def requires_chunks(self) -> bool:
        """Check if this enricher requires chunks."""
        return False  # Requires qa enrichment, not chunks directly

    def configure(self, config: Dict[str, Any]) -> None:
        """
        Configure the enricher.

        Config options:
        - max_content_length: int - Max length of content to include (default: 8000)
        - prompt_template: str - Custom prompt template
        """
        self._config = config
        self._logger.debug(f"Configured with: {config}")

    def process(
        self,
        context: ProcessingContext,
        observability: ObservabilityContext,
    ) -> ProcessingContext:
        """
        Generate full answers for each QA pair.

        Args:
            context: Processing context with QA enrichments.
            observability: Observability context for metrics/tracing.

        Returns:
            Updated context with full_answer enrichments.
        """
        if context.should_skip:
            return context

        # Get QA enrichment data
        qa_data = context.get_enrichment("qa", {})
        if not qa_data:
            self._logger.warning("No QA enrichment found, skipping full answer generation")
            return context

        chunk_qa_list = qa_data.get("chunk_qa", [])
        if not chunk_qa_list:
            self._logger.warning("No chunk_qa data found")
            return context

        # Get full document content
        full_text = ""
        if context.parsed_document:
            full_text = context.parsed_document.full_text

        if not full_text:
            context.add_error(
                step=self.step_type,
                error_type="MissingInput",
                message="No document content available for full answer generation",
                recoverable=True,
            )
            return context

        # Truncate content if too long
        max_content_length = self._config.get("max_content_length", 8000)
        if len(full_text) > max_content_length:
            full_text = full_text[:max_content_length] + "\n...(内容已截断)"

        try:
            full_answers: List[Dict[str, Any]] = []
            total_qa_count = 0

            for chunk_qa in chunk_qa_list:
                if not isinstance(chunk_qa, dict):
                    continue

                chunk_id = chunk_qa.get("chunk_id", "")
                qa_pairs = chunk_qa.get("qa_pairs", [])

                for qa_index, qa in enumerate(qa_pairs):
                    if not isinstance(qa, dict):
                        continue

                    question = qa.get("question", "")
                    if not question:
                        continue

                    total_qa_count += 1

                    # Generate full answer
                    try:
                        prompt = self._get_prompt(question, full_text)
                        full_answer = self._llm_client.chat(prompt)

                        full_answers.append({
                            "chunk_id": chunk_id,
                            "qa_index": qa_index,
                            "question": question,
                            "short_answer": qa.get("answer", ""),
                            "full_answer": full_answer,
                        })

                        self._logger.debug(
                            f"Generated full answer for Q: {question[:50]}..."
                        )

                    except Exception as e:
                        self._logger.warning(
                            f"Failed to generate full answer for question: {e}"
                        )
                        full_answers.append({
                            "chunk_id": chunk_id,
                            "qa_index": qa_index,
                            "question": question,
                            "short_answer": qa.get("answer", ""),
                            "full_answer": "",  # Empty on error
                            "error": str(e),
                        })

            # Store results
            context.set_enrichment(
                self.enrichment_type,
                {
                    "answers": full_answers,
                    "total_count": len(full_answers),
                    "success_count": sum(1 for a in full_answers if a.get("full_answer")),
                },
            )

            self._logger.info(
                f"Generated {len(full_answers)} full answers for {total_qa_count} QA pairs"
            )

        except Exception as e:
            context.add_error(
                step=self.step_type,
                error_type=type(e).__name__,
                message=f"Failed to generate full answers: {e}",
                recoverable=True,
            )
            self._logger.error(f"Full answer generation error: {e}")

        return context

    def _get_prompt(self, question: str, content: str) -> str:
        """Build the prompt for full answer generation."""
        template = self._config.get("prompt_template", self.PROMPT_TEMPLATE)
        return template.format(question=question, content=content)
