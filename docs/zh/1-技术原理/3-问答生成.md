# 问答生成

本章节介绍 gc-qa-rag 智能问答系统中“生成（Generation）”阶段的核心原理与实现细节。该阶段的目标是：**基于用户问题和检索到的高相关知识，利用大语言模型生成自然流畅、综合性概述的答案**。

## 1. 生成流程概述

问答生成阶段位于 RAG（Retrieval-Augmented Generation）三阶段架构的最后一环。其主要流程如下：

1. 用户输入问题，系统完成检索，获得 TopK（如 8 条）高相关知识条目（包含问题、答案、详细解释、摘要、原文链接等元数据）；
2. 系统将用户问题与检索结果整合，构造标准化提示词（Prompt），输入给大语言模型（LLM）；
3. LLM 综合上下文与知识库内容，生成最终答案，并可附带原文链接、详细解释等信息返回给用户。

## 2. 输入结构与提示词设计

生成环节的输入主要包括：

-   **用户问题**（User Input）：当前用户的自然语言提问；
-   **检索结果**（Hits）：由混合检索与 RRF 融合排序得到的高相关知识条目列表。

系统采用结构化提示词模板，将上述信息拼接后输入给 LLM。例如：

```python
"""
你正在和用户对话，请综合参考上下文以及下面的用户问题和知识库检索结果，回答用户的问题。回答时附上文档链接。
## 用户问题
{keyword}

## 知识库检索结果
{hits_text}
"""
```

其中，`hits_text`为检索结果的 JSON 序列化内容，包含每条知识的“Question”、“Answer”、“FullAnswer”、“Summary”、“Url”等字段。

## 3. 生成服务实现

系统通过异步方式调用 LLM 服务，支持流式输出，提升响应速度和用户体验。主要实现逻辑如下：

-   组装消息体（messages），包括 system prompt 和用户 prompt；
-   调用 LLM 的 chat/completions 接口，设置合理的 temperature、top_p 等参数，平衡创造性与准确性；
-   采用流式（stream=True）方式获取模型输出，边生成边返回前端。

相关代码实现见 `ragapp/services/summary.py`：

```python
async def summary_hits(keyword, messages, hits):
    hits_text = json.dumps(hits, ensure_ascii=False, default=vars)
    hits_prompt = f"""你正在和用户对话，请综合参考上下文以及下面的用户问题和知识库检索结果，回答用户的问题。回答时附上文档链接。
    ## 用户问题
    {keyword}
    ## 知识库检索结果
    {hits_text}
    """
    ...
    return chat(messages_with_hits)
```

## 4. 思考模式（Reasoning Mode）

为增强答案的透明度与可信度，系统支持“思考模式”输出。此模式下，LLM 会先输出推理过程，再给出最终答案。例如：

-   以 `> ` 前缀输出推理步骤；
-   用分隔符 `---` 区分推理与正式答案。

相关实现见 `ragapp/services/think.py`，适配支持 reasoning_content 的大模型（如 deepseek-R1）：

```python
async def think(messages):
    ...
    async for chunk in completion:
        reasoning_content = chunk.choices[0].delta.reasoning_content
        ...
        content = chunk.choices[0].delta.content
        ...
```

## 5. 多轮对话与问题重写

系统支持多轮对话场景，能够根据历史对话自动识别用户当前真正想问的问题。具体流程为：

1. 收集历史对话内容，输入给 LLM 问题生成器；
2. LLM 输出用户的真实意图问题，作为检索与生成的输入。

相关实现见 `ragapp/services/query.py`：

```python
async def chat_for_query(contents):
    prompt = f"""你是一个问题生成器，你需要从下面的对话中识别出用户想要查询的问题，直接输出该文本，该文本将用于在知识库中检索相关知识。
    ## 对话内容
    {contents}
    """
    ...
    return chat(messages)
```

## 6. 生成结果结构

最终生成的答案不仅包括直接回答，还可包含：

-   相关知识条目的原文链接（Url）、标题（Title）、分类（Category）等元数据；
-   详细解释（FullAnswer）、上下文摘要（Summary）等辅助信息；
-   用户可选的推理过程（思考模式）。

## 7. 技术优势与工程实践

-   **结构化 Prompt 设计**：确保 LLM 能充分利用检索到的知识，提升答案准确率；
-   **流式输出**：优化用户体验，减少等待时间；
-   **多轮对话支持**：提升复杂场景下的问答能力；
-   **推理透明化**：增强用户信任感，便于结果溯源。
