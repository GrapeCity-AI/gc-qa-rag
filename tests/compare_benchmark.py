#!/usr/bin/env python3
"""
å¯¹æ¯”è„šæœ¬B - æ¯”è¾ƒPostgreSQL+pgvector vs MySQL+Qdrantåœ¨ç”µå­è¡¨æ ¼æŠ€æœ¯æ–‡æ¡£é—®ç­”ä¸Šçš„æ€§èƒ½

ä¸“é—¨é’ˆå¯¹SpreadJSæŠ€æœ¯é—®é¢˜çš„benchmarkåˆ†æ

ç”¨æ³•ï¼š
python compare_benchmark.py result_pgvector.json result_qdrant.json
"""

import json
import sys
from typing import Dict, List
import statistics

def load_result_file(filename: str) -> Dict:
    """åŠ è½½ç»“æœæ–‡ä»¶"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {filename}")
        return None
    except json.JSONDecodeError:
        print(f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯: {filename}")
        return None

def extract_metrics(result_data: Dict) -> Dict:
    """æå–å…³é”®æŒ‡æ ‡"""
    if not result_data or "error" in result_data:
        return None

    test_info = result_data.get("test_info", {})
    detailed_results = result_data.get("detailed_results", [])

    # æå–å“åº”æ—¶é—´
    successful_results = [r for r in detailed_results if r["success"]]
    response_times = [r["response_time_ms"] for r in successful_results]

    # æå–ç­”æ¡ˆæ•°é‡
    hits_counts = [r["hits_count"] for r in successful_results]

    return {
        "total_questions": test_info.get("total_questions", 0),
        "successful_queries": test_info.get("successful_queries", 0),
        "success_rate": test_info.get("success_rate", 0),
        "avg_response_time": test_info.get("average_response_time_ms", 0),
        "response_times": response_times,
        "avg_hits_count": statistics.mean(hits_counts) if hits_counts else 0,
        "p50_response_time": statistics.median(response_times) if response_times else 0,
        "p95_response_time": statistics.quantiles(response_times, n=20)[18] if len(response_times) > 5 else 0,  # 95%
        "min_response_time": min(response_times) if response_times else 0,
        "max_response_time": max(response_times) if response_times else 0,
        # å‡†ç¡®åº¦æŒ‡æ ‡
        "questions_with_ground_truth": test_info.get("questions_with_ground_truth", 0),
        "avg_document_match_rate": test_info.get("average_document_match_rate", 0),
        "avg_answer_similarity": test_info.get("average_answer_similarity", 0),
        "avg_accuracy_score": test_info.get("average_accuracy_score", 0),
    }

def generate_benchmark_report(pgvector_data: Dict, qdrant_data: Dict) -> Dict:
    """ç”Ÿæˆbenchmarkå¯¹æ¯”æŠ¥å‘Š"""
    pg_metrics = extract_metrics(pgvector_data)
    qd_metrics = extract_metrics(qdrant_data)

    if not pg_metrics or not qd_metrics:
        return {"error": "æ— æ³•æå–æŒ‡æ ‡æ•°æ®"}

    # è®¡ç®—æ€§èƒ½å·®å¼‚
    def calc_diff_percent(pg_val, qd_val):
        if qd_val == 0:
            return 0
        return round(((pg_val - qd_val) / qd_val) * 100, 1)

    def determine_winner(pg_val, qd_val, lower_is_better=True):
        if lower_is_better:
            return "pgvector" if pg_val < qd_val else "qdrant"
        else:
            return "pgvector" if pg_val > qd_val else "qdrant"

    benchmark = {
        "test_overview": {
            "pgvector": {
                "total_questions": pg_metrics["total_questions"],
                "successful_queries": pg_metrics["successful_queries"],
                "success_rate": f"{pg_metrics['success_rate']:.1%}"
            },
            "qdrant": {
                "total_questions": qd_metrics["total_questions"],
                "successful_queries": qd_metrics["successful_queries"],
                "success_rate": f"{qd_metrics['success_rate']:.1%}"
            }
        },

        "performance_comparison": {
            "average_response_time": {
                "pgvector_ms": pg_metrics["avg_response_time"],
                "qdrant_ms": qd_metrics["avg_response_time"],
                "difference_percent": calc_diff_percent(pg_metrics["avg_response_time"], qd_metrics["avg_response_time"]),
                "winner": determine_winner(pg_metrics["avg_response_time"], qd_metrics["avg_response_time"])
            },

            "p95_response_time": {
                "pgvector_ms": pg_metrics["p95_response_time"],
                "qdrant_ms": qd_metrics["p95_response_time"],
                "difference_percent": calc_diff_percent(pg_metrics["p95_response_time"], qd_metrics["p95_response_time"]),
                "winner": determine_winner(pg_metrics["p95_response_time"], qd_metrics["p95_response_time"])
            },

            "average_hits_count": {
                "pgvector": round(pg_metrics["avg_hits_count"], 1),
                "qdrant": round(qd_metrics["avg_hits_count"], 1),
                "difference_percent": calc_diff_percent(pg_metrics["avg_hits_count"], qd_metrics["avg_hits_count"], ),
                "winner": determine_winner(pg_metrics["avg_hits_count"], qd_metrics["avg_hits_count"], lower_is_better=False)
            }
        },

        "accuracy_comparison": {
            "document_match_rate": {
                "pgvector": round(pg_metrics["avg_document_match_rate"], 3),
                "qdrant": round(qd_metrics["avg_document_match_rate"], 3),
                "difference_percent": calc_diff_percent(pg_metrics["avg_document_match_rate"], qd_metrics["avg_document_match_rate"]),
                "winner": determine_winner(pg_metrics["avg_document_match_rate"], qd_metrics["avg_document_match_rate"], lower_is_better=False)
            },

            "answer_similarity": {
                "pgvector": round(pg_metrics["avg_answer_similarity"], 3),
                "qdrant": round(qd_metrics["avg_answer_similarity"], 3),
                "difference_percent": calc_diff_percent(pg_metrics["avg_answer_similarity"], qd_metrics["avg_answer_similarity"]),
                "winner": determine_winner(pg_metrics["avg_answer_similarity"], qd_metrics["avg_answer_similarity"], lower_is_better=False)
            },

            "overall_accuracy": {
                "pgvector": round(pg_metrics["avg_accuracy_score"], 3),
                "qdrant": round(qd_metrics["avg_accuracy_score"], 3),
                "difference_percent": calc_diff_percent(pg_metrics["avg_accuracy_score"], qd_metrics["avg_accuracy_score"]),
                "winner": determine_winner(pg_metrics["avg_accuracy_score"], qd_metrics["avg_accuracy_score"], lower_is_better=False)
            },

            "questions_evaluated": {
                "pgvector": pg_metrics["questions_with_ground_truth"],
                "qdrant": qd_metrics["questions_with_ground_truth"]
            }
        },

        "detailed_metrics": {
            "pgvector": pg_metrics,
            "qdrant": qd_metrics
        }
    }

    return benchmark

def print_benchmark_table(benchmark: Dict):
    """æ‰“å°benchmarkå¯¹æ¯”è¡¨æ ¼"""
    if "error" in benchmark:
        print(f"âŒ {benchmark['error']}")
        return

    print("\n" + "="*80)
    print("ğŸ† PostgreSQL+pgvector vs MySQL+Qdrant Benchmark æŠ¥å‘Š")
    print("ğŸ“Š ç”µå­è¡¨æ ¼æŠ€æœ¯æ–‡æ¡£é—®ç­”æ€§èƒ½å¯¹æ¯”")
    print("="*80)

    # æµ‹è¯•æ¦‚è§ˆ
    overview = benchmark["test_overview"]
    print(f"\nğŸ“Š æµ‹è¯•æ¦‚è§ˆ")
    print(f"{'æŒ‡æ ‡':<20} {'pgvector':<15} {'qdrant':<15}")
    print("-" * 50)
    print(f"{'æ€»é—®é¢˜æ•°':<20} {overview['pgvector']['total_questions']:<15} {overview['qdrant']['total_questions']:<15}")
    print(f"{'æˆåŠŸæŸ¥è¯¢æ•°':<20} {overview['pgvector']['successful_queries']:<15} {overview['qdrant']['successful_queries']:<15}")
    print(f"{'æˆåŠŸç‡':<20} {overview['pgvector']['success_rate']:<15} {overview['qdrant']['success_rate']:<15}")

    # æ€§èƒ½å¯¹æ¯”
    perf = benchmark["performance_comparison"]
    print(f"\nğŸš€ æ€§èƒ½å¯¹æ¯”")
    print("-" * 70)

    # å¹³å‡å“åº”æ—¶é—´
    avg_rt = perf["average_response_time"]
    print(f"å¹³å‡å“åº”æ—¶é—´:")
    print(f"  pgvector: {avg_rt['pgvector_ms']:.1f} ms")
    print(f"  qdrant:   {avg_rt['qdrant_ms']:.1f} ms")
    print(f"  å·®å¼‚:     {avg_rt['difference_percent']:+.1f}% (è·èƒœè€…: {avg_rt['winner']})")

    # P95å“åº”æ—¶é—´
    p95_rt = perf["p95_response_time"]
    print(f"\nP95å“åº”æ—¶é—´:")
    print(f"  pgvector: {p95_rt['pgvector_ms']:.1f} ms")
    print(f"  qdrant:   {p95_rt['qdrant_ms']:.1f} ms")
    print(f"  å·®å¼‚:     {p95_rt['difference_percent']:+.1f}% (è·èƒœè€…: {p95_rt['winner']})")

    # å¹³å‡ç»“æœæ•°é‡
    hits = perf["average_hits_count"]
    print(f"\nå¹³å‡ç»“æœæ•°é‡:")
    print(f"  pgvector: {hits['pgvector']:.1f} ä¸ª")
    print(f"  qdrant:   {hits['qdrant']:.1f} ä¸ª")
    print(f"  å·®å¼‚:     {hits['difference_percent']:+.1f}% (è·èƒœè€…: {hits['winner']})")

        # å‡†ç¡®åº¦å¯¹æ¯”
    accuracy = benchmark.get("accuracy_comparison", {})
    if accuracy and accuracy.get("questions_evaluated", {}).get("pgvector", 0) > 0:
        print(f"\nğŸ¯ å‡†ç¡®åº¦å¯¹æ¯” (åŸºäºæ ‡å‡†ç­”æ¡ˆè¯„ä¼°)")
        print("-" * 50)

        doc_match = accuracy["document_match_rate"]
        print(f"æ–‡æ¡£åŒ¹é…ç‡:")
        print(f"  pgvector: {doc_match['pgvector']:.1%}")
        print(f"  qdrant:   {doc_match['qdrant']:.1%}")
        print(f"  å·®å¼‚:     {doc_match['difference_percent']:+.1f}% (è·èƒœè€…: {doc_match['winner']})")

        ans_sim = accuracy["answer_similarity"]
        print(f"\nç­”æ¡ˆç›¸ä¼¼åº¦:")
        print(f"  pgvector: {ans_sim['pgvector']:.1%}")
        print(f"  qdrant:   {ans_sim['qdrant']:.1%}")
        print(f"  å·®å¼‚:     {ans_sim['difference_percent']:+.1f}% (è·èƒœè€…: {ans_sim['winner']})")

        overall = accuracy["overall_accuracy"]
        print(f"\nç»¼åˆå‡†ç¡®åº¦:")
        print(f"  pgvector: {overall['pgvector']:.1%}")
        print(f"  qdrant:   {overall['qdrant']:.1%}")
        print(f"  å·®å¼‚:     {overall['difference_percent']:+.1f}% (è·èƒœè€…: {overall['winner']})")

        questions_eval = accuracy["questions_evaluated"]
        print(f"\nè¯„ä¼°é—®é¢˜æ•°: {questions_eval['pgvector']} vs {questions_eval['qdrant']}")

    # æ€»ç»“
    print(f"\nğŸ† Benchmark æ€»ç»“")
    print("-" * 30)

    # æ€§èƒ½æŒ‡æ ‡
    perf_winners = [avg_rt['winner'], p95_rt['winner'], hits['winner']]

    # å‡†ç¡®åº¦æŒ‡æ ‡
    accuracy_winners = []
    if accuracy and accuracy.get("questions_evaluated", {}).get("pgvector", 0) > 0:
        accuracy_winners = [
            accuracy["document_match_rate"]['winner'],
            accuracy["answer_similarity"]['winner'],
            accuracy["overall_accuracy"]['winner']
        ]

    all_winners = perf_winners + accuracy_winners
    pgvector_wins = all_winners.count('pgvector')
    qdrant_wins = all_winners.count('qdrant')

    print(f"æ€§èƒ½ç»´åº¦: pgvector {perf_winners.count('pgvector')}/3, qdrant {perf_winners.count('qdrant')}/3")
    if accuracy_winners:
        print(f"å‡†ç¡®åº¦ç»´åº¦: pgvector {accuracy_winners.count('pgvector')}/3, qdrant {accuracy_winners.count('qdrant')}/3")

    if pgvector_wins > qdrant_wins:
        print("ğŸ¥‡ ç»¼åˆè·èƒœè€…: PostgreSQL+pgvector")
    elif qdrant_wins > pgvector_wins:
        print("ğŸ¥‡ ç»¼åˆè·èƒœè€…: MySQL+Qdrant")
    else:
        print("ğŸ¤ ç»¼åˆè¡¨ç°ç›¸å½“")

    print("="*80)

def save_benchmark_report(benchmark: Dict, filename: str):
    """ä¿å­˜benchmarkæŠ¥å‘Š"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(benchmark, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {filename}")

def main():
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python compare_benchmark.py <pgvectorç»“æœæ–‡ä»¶> <qdrantç»“æœæ–‡ä»¶>")
        print("ç¤ºä¾‹: python compare_benchmark.py result_pgvector.json result_qdrant.json")
        print("è¯´æ˜: å¯¹æ¯”ä¸¤ç§æ¶æ„åœ¨SpreadJSæŠ€æœ¯æ–‡æ¡£é—®ç­”ä¸Šçš„æ€§èƒ½")
        sys.exit(1)

    pgvector_file = sys.argv[1]
    qdrant_file = sys.argv[2]

    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•ç»“æœ...")
    pgvector_data = load_result_file(pgvector_file)
    qdrant_data = load_result_file(qdrant_file)

    if not pgvector_data or not qdrant_data:
        sys.exit(1)

    # ç”ŸæˆbenchmarkæŠ¥å‘Š
    benchmark = generate_benchmark_report(pgvector_data, qdrant_data)

    # æ‰“å°æŠ¥å‘Š
    print_benchmark_table(benchmark)

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    save_benchmark_report(benchmark, "benchmark_report.json")

if __name__ == "__main__":
    main()
