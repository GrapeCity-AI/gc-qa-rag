#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæœç´¢æ’åæµ‹è¯•è„šæœ¬ - ä¸“æ³¨äºæœç´¢ç»“æœæ”¶é›†
è·å–å‰8ä¸ªæ’åç»“æœï¼Œå®Œæ•´ä¿å­˜RAGæ£€ç´¢æ•°æ®
"""

import json
import time
import asyncio
import aiohttp
import argparse
from typing import List, Dict, Any
from datetime import datetime
import uuid
from ground_truth_parser import parse_qa_list, get_question_ground_truth

class SearchRankingTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.ground_truth = parse_qa_list()
        self.session_id = str(uuid.uuid4())
        print(f"âœ… åŠ è½½äº† {len(self.ground_truth)} ä¸ªæ ‡å‡†ç­”æ¡ˆç”¨äºå‡†ç¡®åº¦è¯„ä¼°")
        print(f"ğŸ”‘ ä¼šè¯ID: {self.session_id}")

    def load_questions(self, questions_file: str = 'test_queries.json') -> List[str]:
        """åŠ è½½é—®é¢˜åˆ—è¡¨"""
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('queries', [])
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½é—®é¢˜æ–‡ä»¶ {questions_file}: {e}")
            return []

    def extract_source_documents(self, hits: List[Dict]) -> List[str]:
        """ä»hitsä¸­æå–æºæ–‡æ¡£åç§°"""
        source_docs = []
        for hit in hits:
            payload = hit.get('payload', {})
            doc_name = (
                payload.get('title') or
                payload.get('source_doc') or
                payload.get('file_name') or
                payload.get('url', '').split('/')[-1]
            )
            if doc_name and doc_name not in source_docs:
                source_docs.append(doc_name)
        return source_docs

    def calculate_accuracy_metrics(self, question: str, hits: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—å‡†ç¡®åº¦æŒ‡æ ‡"""
        gt_info = get_question_ground_truth(question, self.ground_truth)

        if not gt_info:
            return {"has_ground_truth": False}

        # è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œå‚è€ƒæ–‡æ¡£
        retrieved_docs = self.extract_source_documents(hits)
        reference_docs = gt_info.get('reference_docs', [])

        # æ–‡æ¡£åŒ¹é…ç‡è®¡ç®—
        if not reference_docs:
            doc_match_rate = 1.0
        else:
            matched_docs = set(retrieved_docs) & set(reference_docs)
            doc_match_rate = len(matched_docs) / len(reference_docs)

        return {
            "has_ground_truth": True,
            "reference_docs": reference_docs,
            "retrieved_docs": retrieved_docs,
            "document_match_rate": round(doc_match_rate, 3),
            "matched_documents": list(set(retrieved_docs) & set(reference_docs)),
            "missing_documents": list(set(reference_docs) - set(retrieved_docs)),
            "extra_documents": list(set(retrieved_docs) - set(reference_docs)),
            "standard_answer": gt_info.get('standard_answer', '')
        }

    async def search_single_question(self, question: str, session_index: int = 0) -> Dict[str, Any]:
        """æœç´¢å•ä¸ªé—®é¢˜ï¼Œè·å–æ’åå‰8çš„ç»“æœ"""
        start_time = time.perf_counter()

        try:
            async with aiohttp.ClientSession() as session:
                # æ¨¡æ‹Ÿå‰ç«¯è¯·æ±‚æ ¼å¼
                payload = {
                    "keyword": question,
                    "mode": "chat",
                    "product": "bbb",
                    "session_id": self.session_id,
                    "session_index": session_index
                }

                print(f"ğŸ” [DEBUG] æœç´¢è¯·æ±‚: {question[:50]}...")

                async with session.post(
                    f"{self.base_url}/search/",
                    json=payload,
                    headers={
                        "accept": "*/*",
                        "content-type": "application/json"
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:

                    end_time = time.perf_counter()
                    response_time = (end_time - start_time) * 1000

                    print(f"ğŸ“¡ [DEBUG] å“åº”çŠ¶æ€: {response.status}, æ—¶é—´: {response_time:.2f}ms")

                    if response.status == 200:
                        result = await response.json()

                        # å¤„ç†å“åº”æ ¼å¼
                        if isinstance(result, list):
                            hits = result
                        elif isinstance(result, dict):
                            hits = result.get('hits', result.get('data', []))
                        else:
                            hits = []

                        # å–å‰8ä¸ªç»“æœ
                        top_hits = hits[:8]
                        print(f"âœ… [DEBUG] è·å– {len(top_hits)} ä¸ªæœç´¢ç»“æœ")

                        # æå–å®Œæ•´çš„æœç´¢ç»“æœä¿¡æ¯
                        search_results = []
                        for i, hit in enumerate(top_hits, 1):
                            payload_data = hit.get('payload', {})
                            search_results.append({
                                "rank": i,
                                "score": hit.get('score', 0),
                                "answer": payload_data.get('answer', ''),
                                "title": payload_data.get('title', ''),
                                "url": payload_data.get('url', ''),
                                "summary": payload_data.get('summary', ''),
                                "question": payload_data.get('question', ''),
                                "product": payload_data.get('product', ''),
                                "category": payload_data.get('category', ''),
                                "file_index": payload_data.get('file_index', ''),
                                "collection_category": payload_data.get('collection_category', '')
                            })

                        # è®¡ç®—å‡†ç¡®åº¦æŒ‡æ ‡
                        accuracy_metrics = self.calculate_accuracy_metrics(question, top_hits)

                        # æå–æ‰€æœ‰å‚è€ƒæ–‡æ¡£
                        all_source_docs = self.extract_source_documents(top_hits)

                        return {
                            "question": question,
                            "success": True,
                            "response_time_ms": round(response_time, 2),
                            "total_hits": len(hits),
                            "top_8_results": search_results,  # å®Œæ•´çš„å‰8ä¸ªç»“æœ
                            "all_source_docs": all_source_docs,
                            "accuracy_metrics": accuracy_metrics,
                            "session_info": {
                                "session_id": self.session_id,
                                "session_index": session_index
                            }
                        }
                    else:
                        error_text = await response.text()
                        print(f"âŒ [DEBUG] HTTPé”™è¯¯ {response.status}: {error_text[:200]}...")

                        return {
                            "question": question,
                            "success": False,
                            "response_time_ms": round(response_time, 2),
                            "error": f"HTTP {response.status}: {error_text[:100]}",
                            "total_hits": 0,
                            "top_8_results": [],
                            "all_source_docs": [],
                            "accuracy_metrics": self.calculate_accuracy_metrics(question, []),
                            "session_info": {
                                "session_id": self.session_id,
                                "session_index": session_index
                            }
                        }

        except Exception as e:
            end_time = time.perf_counter()
            response_time = (end_time - start_time) * 1000
            print(f"âŒ [DEBUG] è¯·æ±‚å¼‚å¸¸: {str(e)}")

            return {
                "question": question,
                "success": False,
                "response_time_ms": round(response_time, 2),
                "error": str(e),
                "total_hits": 0,
                "top_8_results": [],
                "all_source_docs": [],
                "accuracy_metrics": self.calculate_accuracy_metrics(question, []),
                "session_info": {
                    "session_id": self.session_id,
                    "session_index": session_index
                }
            }

    async def run_all_searches(self, questions_file: str = 'test_queries.json') -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æœç´¢"""
        questions = self.load_questions(questions_file)

        if not questions:
            return {"error": "æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•é—®é¢˜"}

        print(f"ğŸ“‹ å¼€å§‹æœç´¢ {len(questions)} ä¸ªSpreadJSæŠ€æœ¯é—®é¢˜...")
        results = []

        for i, question in enumerate(questions):
            print(f"  {i+1}/{len(questions)}: {question[:80]}...")
            result = await self.search_single_question(question, session_index=i)
            results.append(result)

            # ç®€çŸ­çš„è¿›åº¦åé¦ˆ
            if result['success']:
                top_result = result['top_8_results'][0] if result['top_8_results'] else {}
                preview = top_result.get('answer', '')[:100] + ("..." if len(top_result.get('answer', '')) > 100 else "")
                print(f"    âœ… {result['response_time_ms']:.0f}ms, {result['total_hits']}ä¸ªç»“æœ - {preview}")
            else:
                print(f"    âŒ {result.get('error', 'Unknown error')}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]

        if successful_results:
            response_times = [r['response_time_ms'] for r in successful_results]
            total_hits = [r['total_hits'] for r in successful_results]
            avg_response_time = sum(response_times) / len(response_times)
            avg_hits = sum(total_hits) / len(total_hits)
        else:
            avg_response_time = avg_hits = 0

        # å‡†ç¡®åº¦ç»Ÿè®¡
        accuracy_results = [r for r in successful_results if r.get("accuracy_metrics", {}).get("has_ground_truth", False)]
        if accuracy_results:
            avg_doc_match_rate = sum([r["accuracy_metrics"]["document_match_rate"] for r in accuracy_results]) / len(accuracy_results)
        else:
            avg_doc_match_rate = 0.0

        print(f"\nâœ… æœç´¢å®Œæˆï¼æˆåŠŸ: {len(successful_results)}/{len(questions)}")
        print(f"ğŸ“Š å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}ms")
        print(f"ğŸ“Š å¹³å‡æœç´¢ç»“æœ: {avg_hits:.1f}ä¸ª")
        print(f"ğŸ¯ å‡†ç¡®åº¦ç»Ÿè®¡ ({len(accuracy_results)}ä¸ªé—®é¢˜æœ‰æ ‡å‡†ç­”æ¡ˆ):")
        print(f"   æ–‡æ¡£åŒ¹é…ç‡: {avg_doc_match_rate:.1%}")

        return {
            "test_info": {
                "timestamp": time.time(),
                "test_type": "search_ranking",
                "session_id": self.session_id,
                "total_questions": len(questions),
                "successful_queries": len(successful_results),
                "failed_queries": len(failed_results),
                "success_rate": len(successful_results) / len(questions) if questions else 0,
                "average_response_time_ms": avg_response_time,
                "average_hits_count": avg_hits,
                "questions_with_ground_truth": len(accuracy_results),
                "average_document_match_rate": avg_doc_match_rate
            },
            "detailed_results": results
        }

def main():
    parser = argparse.ArgumentParser(description='RAGæœç´¢æ’åæµ‹è¯•è„šæœ¬')
    parser.add_argument('--output', help='è¾“å‡ºç»“æœæ–‡ä»¶å(JSONæ ¼å¼)', default='search_ranking_results.json')
    parser.add_argument('--url', default='http://localhost:8000', help='RAGæœåŠ¡URL')
    parser.add_argument('--test', action='store_true', help='ä»…æµ‹è¯•å•ä¸ªæŸ¥è¯¢')
    parser.add_argument('--question', help='æµ‹è¯•å•ä¸ªé—®é¢˜çš„å†…å®¹')
    parser.add_argument('--questions-file', default='test_queries.json', help='é—®é¢˜æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    async def run_test():
        tester = SearchRankingTester(args.url)

        if args.test:
            # æµ‹è¯•å•ä¸ªé—®é¢˜
            question = args.question if args.question else "å¦‚ä½•åˆ›å»ºç›¸æœºè§†å›¾"
            print(f"ğŸ§ª [TEST] æµ‹è¯•å•ä¸ªæœç´¢: {question}...")
            result = await tester.search_single_question(question)

            print(f"\nğŸ“Š æœç´¢ç»“æœ:")
            print(f"   æˆåŠŸ: {result['success']}")
            print(f"   å“åº”æ—¶é—´: {result['response_time_ms']:.2f}ms")
            print(f"   æ€»ç»“æœæ•°: {result['total_hits']}")
            print(f"   å‰8ç»“æœ: {len(result['top_8_results'])}ä¸ª")
            if result['top_8_results']:
                print(f"   ç¬¬1åç­”æ¡ˆ: {result['top_8_results'][0]['answer'][:100]}...")
            return

        # è¿è¡Œå®Œæ•´æœç´¢æµ‹è¯•
        results = await tester.run_all_searches(args.questions_file)

        if "error" in results:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results['error']}")
            return

        # ä¿å­˜JSONç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        print("ğŸ’¡ ä½¿ç”¨Markdownç”Ÿæˆå™¨åˆ›å»ºè¯¦ç»†æŠ¥å‘Š:")
        print(f"   python generate_markdown_report.py {args.output}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_test())

if __name__ == "__main__":
    main()
